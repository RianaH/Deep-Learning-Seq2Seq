{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8FdySj8ktNO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from numpy.random.mtrand import random_integers\n",
        "from scipy.sparse import rand\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "# Setup - you have to ensure this path is correct for your google drive too\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder_path = '/content/drive/My Drive/Neuro Project/Rat Neural Data/Valid Trials Data'\n",
        "os.listdir(folder_path)\n",
        "\n",
        "##########DEFINE NN AND FUNCTIONS:\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "def poisson_loss(y_pred, y_true):\n",
        "    # Ensuring non-negative predictions\n",
        "    y_pred = torch.clamp(y_pred, min=0, max=float('inf'))\n",
        "    # Adding epsilon to avoid log(0)\n",
        "    return torch.mean(y_pred - y_true * torch.log(y_pred + 1e-8))\n",
        "\n",
        "#specify the network structure:\n",
        "# create model\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2,hidden_size3,hidden_size4,output_size):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.input_size = input_size #400\n",
        "        self.hidden_size1 = hidden_size1 #?? 300\n",
        "        self.hidden_size2 = hidden_size2 #?? 200\n",
        "        self.hidden_size3 = hidden_size3 #?? 100\n",
        "        self.hidden_size4 = hidden_size4 #?? 50\n",
        "        self.output_size = output_size #?? 1\n",
        "\n",
        "        self.iToh1 = torch.nn.Linear(input_size, hidden_size1) #400,300\n",
        "        self.h1Toh2 = torch.nn.Linear(hidden_size1, hidden_size2) #300,200\n",
        "        self.h2Toh3 = torch.nn.Linear(hidden_size2, hidden_size3) #200,100\n",
        "        self.h3Toh4 = torch.nn.Linear(hidden_size3, hidden_size4) #100,50\n",
        "        self.h4ToO = torch.nn.Linear(hidden_size4, output_size) #50,1\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        #self.Conv1d = torch.nn.Conv1d(in_channels=13, out_channels=1, kernel_size=1) ##This in concert with other changes will let you select random subset of trials.\n",
        "        self.ReLU = torch.nn.ReLU()\n",
        "        self.LSM = torch.nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #a = self.Conv1d(x)\n",
        "        x = self.LSM(self.iToh1(x)) ###was LSM\n",
        "        x = self.LSM(self.h1Toh2(x)) ##was sigmoid\n",
        "        x = self.LSM(self.h2Toh3(x)) ##was sigmoid\n",
        "        x = self.LSM(self.h3Toh4(x)) ##was sigmoid\n",
        "        x = self.LSM(self.h4ToO(x)) ##was sigmoid #no activation here if using x-entropy loss, bc already has one\n",
        "        return x ###maybe restrict to examples with over 30 neurons.\n",
        "\n",
        "###########DO THE LEARNING\n",
        "PossibleFiles = ['R397-2017-06-09', 'R417-2017-10-28','R417-2017-10-31','R397-2017-06-09', 'R417-2017-10-28','R417-2017-10-31','R417-2017-11-02','R423-2017-08-02','R423-2017-08-04','R423-2017-08-05','R423-2017-08-07','R442-2018-03-17','R442-2018-03-19', 'R442-2018-03-20', 'R442-2018-03-22','R465-2018-08-14','R465-2018-08-15','R465-2018-08-18','R465-2018-08-20','R497-2018-11-01','R497-2018-11-04','R497-2018-11-05','R501-2019-01-04','R501-2019-01-06','R501-2019-01-09','R522-2019-03-13','R522-2019-03-15','R522-2019-03-16','R522-2019-03-18'];\n",
        "FileSets = np.random.choice(len(PossibleFiles), len(PossibleFiles), replace = False)\n",
        "TrainVal = FileSets[0:18] #training examples...~60/40 split.\n",
        "TestVal =  FileSets[18:30]\n",
        "\n",
        "for i in range(0,len(TrainVal)-1): #reserve last file in TrainVal for validation.\n",
        "  # For the regular data, the goal is just to get it into vector form.\n",
        "  Target_df = pd.read_csv(os.path.join(folder_path, PossibleFiles[TrainVal[i]]+\"_KD.csv\"))\n",
        "  Target_Tensor = torch.tensor(Target_df.values)\n",
        "  Target_Tensor = torch.LongTensor(Target_Tensor)\n",
        "\n",
        "  # Goal is to get to Trials X Neurons X Times for the neural data.\n",
        "  HC_df = pd.read_csv(os.path.join(folder_path, PossibleFiles[TrainVal[i]]+'_KDHC.dat'), delimiter=',')\n",
        "  HC_df = HC_df.values.reshape(HC_df.shape[0], int(HC_df.shape[1]/400), 400)\n",
        "  HC_df = np.sum(HC_df,1) ##this is a way to avoid the conv layer.\n",
        "  #try:\n",
        "  #  HC_df = HC_df[:,np.random.choice(HC_df.shape[1], 13, replace = False),:]\n",
        "  #except:\n",
        "  #  break\n",
        "  HC_Tensor = torch.FloatTensor(HC_df)\n",
        "\n",
        "  PF_df = pd.read_csv(os.path.join(folder_path, PossibleFiles[TrainVal[i]]+'_KDPF.dat'), delimiter=',')\n",
        "  PF_df = PF_df.values.reshape(PF_df.shape[0], int(PF_df.shape[1]/400), 400)\n",
        "  PF_df = np.sum(PF_df,1)\n",
        "  #PF_df = PF_df[:,np.random.choice(PF_df.shape[1], 13, replace = False),:]\n",
        "  #try:\n",
        "  #  PF_df = PF_df[:,np.random.choice(PF_df.shape[1], 13, replace = False),:]\n",
        "  #except:\n",
        "  #  break\n",
        "  PF_Tensor = torch.FloatTensor(PF_df)\n",
        "\n",
        "  loader = DataLoader(list(zip(PF_Tensor, Target_Tensor)), shuffle=True, batch_size=20)\n",
        "\n",
        "  #Validation Data:\n",
        "# For the regular data, the goal is just to get it into vector form.\n",
        "  Val_Target_df = pd.read_csv(os.path.join(folder_path, PossibleFiles[TrainVal[-1]]+\"_KD.csv\"))\n",
        "  Val_Target_Tensor = torch.tensor(Val_Target_df.values)\n",
        "  Val_Target_Tensor = torch.LongTensor(Val_Target_Tensor)\n",
        "\n",
        "  # Goal is to get to Trials X Neurons X Times for the neural data.\n",
        "  Val_HC_df = pd.read_csv(os.path.join(folder_path, PossibleFiles[TrainVal[-1]]+'_KDHC.dat'), delimiter=',')\n",
        "  Val_HC_df = Val_HC_df.values.reshape(Val_HC_df.shape[0], int(Val_HC_df.shape[1]/400), 400)\n",
        "  Val_HC_df = np.sum(Val_HC_df,1) ##this is a way to avoid the conv layer.\n",
        "  #try:\n",
        "  #  Val_HC_df = Val_HC_df[:,np.random.choice(Val_HC_df.shape[1], 13, replace = False),:]\n",
        "  #except:\n",
        "  #  break\n",
        "  #Val_HC_Tensor = torch.FloatTensor(Val_HC_df)\n",
        "\n",
        "  Val_PF_df = pd.read_csv(os.path.join(folder_path, PossibleFiles[TrainVal[-1]]+'_KDPF.dat'), delimiter=',')\n",
        "  Val_PF_df = Val_PF_df.values.reshape(Val_PF_df.shape[0], int(Val_PF_df.shape[1]/400), 400)\n",
        "  Val_PF_df = np.sum(Val_PF_df,1)\n",
        "  #PF_df = PF_df[:,np.random.choice(PF_df.shape[1], 13, replace = False),:]\n",
        "  #try:\n",
        "  #  Val_PF_df = Val_PF_df[:,np.random.choice(Val_PF_df.shape[1], 13, replace = False),:]\n",
        "  #except:\n",
        "  #  break\n",
        "  Val_PF_Tensor = torch.FloatTensor(Val_PF_df)\n",
        "\n",
        "  #Now begin with DNN.\n",
        "\n",
        "  best_acc = 0\n",
        "  TrackLoss = []\n",
        "  TrackValLoss = []\n",
        "  for i in range(0,5):\n",
        "    model = Net(400,300,200,100,50,1) #was: 400,300,200,100,1\n",
        "    # training parameters\n",
        "    n_epochs = 180 #180 was the last one you tried that\n",
        "    #loss_fn = torch.nn.PoissonNLLLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) #was .0001\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "      for X_batch, y_batch in loader:\n",
        "          y_pred = model(X_batch)\n",
        "          loss = poisson_loss(y_pred, y_batch) #loss_fn to use built in loss. #50x31x1 pred vs. 50x1 batch ##torch.argmax(y_pred, 1) fixes the misalignment of sizes, but causes another issue: it isn't differentiable, so it has no gradient and will break the backward loss.\n",
        "          TrackLoss.append(loss)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward(retain_graph=True) ##change this option?\n",
        "          #scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.60, total_iters=30)....maybe eliminate b/c is not LR?\n",
        "          optimizer.step() #add a tracker of the validation loss. You need to track both the training and val loss.\n",
        "\n",
        "    model.eval()\n",
        "    ###Now test on validation set:\n",
        "    y_pred = model(Val_PF_Tensor)\n",
        "    ce = poisson_loss(y_pred,Val_Target_Tensor)\n",
        "    TrackValLoss.append(ce)\n",
        "    current_acc = (torch.round(y_pred).int() == Val_Target_Tensor).float().mean()\n",
        "    if current_acc > best_acc:\n",
        "      best_acc = current_acc\n",
        "      best_model = model\n",
        "\n",
        "###Then Test the Best Performing Model.\n",
        "TestAccList = [];\n",
        "NIR_Num = np.array([]);\n",
        "NIR_Den = np.array([]);\n",
        "for i in range(0,len(TestVal)): #reserve last file in TrainVal for validation.\n",
        "  # For the regular data, the goal is just to get it into vector form.\n",
        "  Target_df = pd.read_csv(os.path.join(folder_path, PossibleFiles[TestVal[i]]+\"_KD.csv\"))\n",
        "  Target_Tensor = torch.tensor(Target_df.values)\n",
        "  Target_Tensor = torch.LongTensor(Target_Tensor)\n",
        "  NIR_Num = np.append(NIR_Num,sum(Target_df.values==0))\n",
        "  NIR_Den = np.append(NIR_Den,len(Target_df.values))\n",
        "\n",
        "  # Goal is to get to Trials X Neurons X Times for the neural data.\n",
        "  HC_df = pd.read_csv(os.path.join(folder_path, PossibleFiles[TestVal[i]]+'_KDHC.dat'), delimiter=',')\n",
        "  HC_df = HC_df.values.reshape(HC_df.shape[0], int(HC_df.shape[1]/400), 400)\n",
        "  HC_df = np.sum(HC_df,1) ##this is a way to avoid the conv layer.\n",
        "  #HC_df = HC_df[:,np.random.choice(HC_df.shape[1], 13, replace = False),:]...downsample to match min.\n",
        "  HC_Tensor = torch.FloatTensor(HC_df)\n",
        "\n",
        "  PF_df = pd.read_csv(os.path.join(folder_path, PossibleFiles[TestVal[i]]+'_KDPF.dat'), delimiter=',')\n",
        "  PF_df = PF_df.values.reshape(PF_df.shape[0], int(PF_df.shape[1]/400), 400)\n",
        "  PF_df = np.sum(PF_df,1)\n",
        "  #PF_df = PF_df[:,np.random.choice(PF_df.shape[1], 13, replace = False),:]\n",
        "  PF_Tensor = torch.FloatTensor(PF_df)\n",
        "\n",
        "  #test on training data for now, not actually test data:\n",
        "  testy_pred = best_model(HC_Tensor) #adjusting this, and adjusting model(tensor) call will change between HC and PF\n",
        "  test_acc = (torch.round(testy_pred).int() == Target_Tensor).float().mean()\n",
        "  TestAccList.append(test_acc)\n",
        "\n",
        "torch.stack(TestAccList).mean()  #average model performance. #best is: M=.53. SD=.11. Good news: it doesn't seem to predict all 0s or 1s.\n",
        "torch.stack(TestAccList).std()\n",
        "\n",
        "#confusion (one loop only, not averages):\n",
        "Hits = torch.sum(torch.eq(Target_Tensor, torch.round(testy_pred).int() == 1)) #both 1\n",
        "Miss = torch.sum(torch.eq(Target_Tensor, torch.round(testy_pred).int() != 1)) #true 1, pred 0\n",
        "FA = torch.sum(torch.eq(1-Target_Tensor, torch.round(testy_pred).int() == 1)) #true 0, pred 1\n",
        "CRs = torch.sum(torch.eq(1-Target_Tensor, torch.round(testy_pred).int() != 1)) #both 0\n",
        "\n",
        "#NIR:\n",
        "max(sum(NIR_Num)/sum(NIR_Den),1-sum(NIR_Num)/sum(NIR_Den))\n",
        "\n",
        "##try the following: -->set a min of 38 neurons, this will exclude some sessions, but try that. --> Try the random subset of 13 neurons again. Try with 13.\n",
        "#-->record the output with both of these options.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m6Vl40zCpRxD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
